{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qgL5Tjuq1vV6",
    "outputId": "a7af5f6c-24dd-4b7c-8283-97d492cdb60a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pngvCgT0D6zw"
   },
   "source": [
    "*Final Assignment Sample Code*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MpPFqzaCOQQi",
    "outputId": "f313327a-2f21-40ba-e351-8a87b0952870"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "iris_dict = load_iris()\n",
    "X = iris_dict['data']\n",
    "y = iris_dict['target']\n",
    "\n",
    "# shuffle arrays since y values are in order\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X_new, y_new = shuffle(X, y, random_state=0)\n",
    "\n",
    "# Divide samples into train and test \n",
    "\n",
    "n_samples_train = 120 # number of samples for training (--> #samples for testing = len(y_new) - 120 = 30)\n",
    "X_train = X_new[:n_samples_train, :]\n",
    "y_train = y_new[:n_samples_train]\n",
    "\n",
    "X_test = X_new[n_samples_train:, :]\n",
    "y_test = y_new[n_samples_train:]\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('iris_trained_model.pkl', 'wb') as f:\n",
    "    pickle.dump(clf, f)\n",
    "\n",
    "with open('iris_trained_model.pkl', 'rb') as f:\n",
    "    clf_loaded = pickle.load(f)\n",
    "\n",
    "clf_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z775z_sKwy54",
    "outputId": "a886500c-d788-419b-b9cf-d0876d38f5ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mysqlclient\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/df/59cd2fa5e48d0804d213bdcb1acb4d08c403b61c7ff7ed4dd4a6a2deb3f7/mysqlclient-2.0.3.tar.gz (88kB)\n",
      "\r",
      "\u001b[K     |███▊                            | 10kB 14.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████▍                        | 20kB 10.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 30kB 8.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▊                 | 40kB 7.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 51kB 4.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 61kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 71kB 4.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 81kB 5.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 92kB 3.8MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: mysqlclient\n",
      "  Building wheel for mysqlclient (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for mysqlclient: filename=mysqlclient-2.0.3-cp37-cp37m-linux_x86_64.whl size=100113 sha256=d57585ad8b59133c14d4b789a7ff6d8644bbedaaed51bf74f3e6316dee1b4bde\n",
      "  Stored in directory: /root/.cache/pip/wheels/75/ca/e8/ad4e7ce3df18bcd91c7d84dd28c7c08db491a2a2360efed363\n",
      "Successfully built mysqlclient\n",
      "Installing collected packages: mysqlclient\n",
      "Successfully installed mysqlclient-2.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install mysqlclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9RUAFg6EFM3"
   },
   "source": [
    "*Defining Airflow Directory and Creating airflow.cfg*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nD5mkFNydRFX"
   },
   "outputs": [],
   "source": [
    "!export AIRFLOW_HOME=~/airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DC2WUI5HEkpN"
   },
   "source": [
    "*Initialization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sYq_8C-V34IR",
    "outputId": "8dbba7a3-3c85-47e0-a903-918586c034e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB: sqlite:////root/airflow/airflow.db\n",
      "[\u001b[34m2021-06-13 14:47:26,018\u001b[0m] {\u001b[34mdb.py:\u001b[0m695} INFO\u001b[0m - Creating tables\u001b[0m\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "WARNI [airflow.models.crypto] empty cryptography key - values will not be stored encrypted.\n",
      "Initialization done\n"
     ]
    }
   ],
   "source": [
    "!airflow db init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NN03ULcnDWwS"
   },
   "source": [
    " *My Main Code*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "rVxZAmDdfUiI"
   },
   "outputs": [],
   "source": [
    "# Code by Hamide Hadibeik\n",
    "# .................... Import Packages ...........................\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.utils import timezone\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import pickle\n",
    "import boto3\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# ..................... AmazonS3Client Class .......................\n",
    "\n",
    "class AmazonS3Client(object):\n",
    "    _cl = None\n",
    "    def __init__(self):\n",
    "        if not self._cl:\n",
    "            self._cl = boto3.client('S3',\n",
    "                aws_access_key_id='ASIAWZXXBZXUBCOXJD6',\n",
    "                aws_secret_access_key='vqGoB5IgLjmQP4r+H7Y+CivkZ9MDFnDrL9wGeJ4',\n",
    "                aws_session_token='FwoGZXIvYXdzEGAaDE+MCg9UQ4hn0kVFUCLIARC3cCH2IfPKhdrjgcUDCohmMQNlkRfjNWs0lltcqFZ1UnNlFISqfIbufsn6H/4a9gUEXcR0yM04LM0F65e9Wo+CpGJtwlg96JK/kCftDmG+lt9zvJukXPKdwb7wAzWt/7f+aNz3O9ohpctEkK16FmltPD3+bICxgPxKqiLdNYKnBZ4XEbPGA0/1PySJIqsPgjP4nRtfMGKJTvDHsfsLjiwFzuhmeypoxGPjBDNFVrDMUOvEUqpE/JQXJu6D7USlklTm44Yn14noKNH87PYFMi1kxz8FAIx5Tl1TukfahpGSt+tco7UCZuIG26DopnkXMSP/VT8cBXzgpsM/FF=',\n",
    "            )\n",
    "        self.cl = self._cl\n",
    "\n",
    "# ...................... Download and Upload ....................\n",
    "\n",
    "def download(file_name, bucket_name, save_as=None):\n",
    "    s3 = AmazonS3Client().cl\n",
    "    if not save_as:\n",
    "        save_as = file_name\n",
    "    s3.download_file(bucket_name, file_name, save_as)\n",
    "\n",
    "\n",
    "def upload(file_obj, file_name, bucket_name):\n",
    "    s3 = S3ClientSingleton().cl\n",
    "    s3.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=file_name,\n",
    "        Body=file_obj)\n",
    "\n",
    "\n",
    "#.............. Task functions (task1, task2, task3, task4) .............\n",
    "\n",
    "def task1():\n",
    "    s3 = S3ClientSingleton().cl\n",
    "    for s3_samp in s3.list_objects(Bucket='hamideh.ub.training')['Contents']:\n",
    "        filename = s3_samp['Key']\n",
    "        download(filename, 'hamideh.ub.training', filename)\n",
    "\n",
    "\n",
    "def task2():\n",
    "    file = glob.glob(\"train*.csv\")\n",
    "    data = []\n",
    "    for myfile in file:\n",
    "        df = pd.read_csv(myfile, index_col=None, header=0)\n",
    "        data.append(df)\n",
    "    df = pd.concat(data, axis=0, ignore_index=True)\n",
    "    X = df.loc[:, df.columns != 'Species']\n",
    "    y = df.loc[:, df.columns == 'Species']\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X, y)\n",
    "    with open('iris_trained_model.pkl', 'wb') as f:\n",
    "        pickle.dump(clf, f)\n",
    "\n",
    "def task3():\n",
    "    download('predict.csv', 'hamideh.ub.prediction')\n",
    "    my_test_data = pd.read_csv('predict.csv', index_col=None, header=0)\n",
    "    with open('iris_trained_model.pkl', 'rb') as f:\n",
    "        clf_loaded = pickle.load(f)\n",
    "    y_pred = clf_loaded.predict(my_test_data)\n",
    "    my_test_data['Species'] = y_pred\n",
    "    my_test_data.to_csv('_' + 'predict.csv')\n",
    "\n",
    "def task4():\n",
    "    with open('iris_trained_model.pkl', 'rb') as f1:\n",
    "        upload(f1, 'iris_trained_model.pkl', 'hamideh.ub.ml')\n",
    "    with open('predict.csv', 'rb') as f2:\n",
    "        myfiles = 'prediction_{}'.format(datetime.today())\n",
    "        upload(f2, myfiles, 'hamideh.ub.prediction')\n",
    "\n",
    "\n",
    "#............... Arguments of the DAG ...................\n",
    "\n",
    "args = {\n",
    "    'owner': 'Hamideh',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': timezone.utcnow(),\n",
    "    'email': ['airflow@my_first_dag.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "# ................ My DAG ...................\n",
    "\n",
    "dag = DAG(dag_id='MyDag',\n",
    "          default_args=args,\n",
    "          description=\"MyDAG in Final Assignment of This Course\",\n",
    "          schedule_interval='0 20 0 * 1')  # at 8 PM each Monday \n",
    "\n",
    "# ................... Tasks 1-4 .....................\n",
    "\n",
    "op1 = PythonOperator(task_id='Task1',\n",
    "    python_callable=task1,\n",
    "    dag=dag)\n",
    "\n",
    "op2 = PythonOperator(task_id='Task2',\n",
    "    python_callable=task2,\n",
    "    dag=dag)\n",
    "\n",
    "op3 = PythonOperator(task_id='Task3',\n",
    "    python_callable=task3,\n",
    "    dag=dag)\n",
    "\n",
    "op4 = PythonOperator(task_id='Task4',\n",
    "    python_callable=task4,\n",
    "    dag=dag)\n",
    "\n",
    "\n",
    "# ........................ End ................................\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zP-ueyXuDD2B"
   },
   "source": [
    "*Create DAG*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YIDORbQyCwTT",
    "outputId": "7ff0138f-f935-4f3a-f3f3-3a5670d393f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Task(PythonOperator): Task2>"
      ]
     },
     "execution_count": 68,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op1 >> op2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUelUinCC9v6",
    "outputId": "6cd81398-7abb-45ee-8611-ae307196b293"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Task(PythonOperator): Task4>"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[op2, op3] >> op4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cp51n2hEDgXS"
   },
   "source": [
    "*Connecting to localhost:8081*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48tB1DUFpiL8",
    "outputId": "48278025-d9c6-4adb-91f4-91920a6a7e9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ____________       _____________\n",
      " ____    |__( )_________  __/__  /________      __\n",
      "____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /\n",
      "___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /\n",
      " _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/\n",
      "[\u001b[34m2021-06-13 15:02:16,938\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m487} INFO\u001b[0m - Filling up the DagBag from \u001b[01m/dev/null\u001b[22m\u001b[0m\n",
      "Running the Gunicorn Server with:\n",
      "Workers: 4 sync\n",
      "Host: 0.0.0.0:8081\n",
      "Timeout: 120\n",
      "Logfiles: - -\n",
      "Access Logformat: \n",
      "=================================================================            \n",
      "[2021-06-13 15:02:19 +0000] [4126] [INFO] Starting gunicorn 20.1.0\n",
      "[2021-06-13 15:02:19 +0000] [4126] [INFO] Listening at: http://0.0.0.0:8081 (4126)\n",
      "[2021-06-13 15:02:19 +0000] [4126] [INFO] Using worker: sync\n",
      "[2021-06-13 15:02:19 +0000] [4129] [INFO] Booting worker with pid: 4129\n",
      "[2021-06-13 15:02:19 +0000] [4130] [INFO] Booting worker with pid: 4130\n",
      "[2021-06-13 15:02:19 +0000] [4131] [INFO] Booting worker with pid: 4131\n",
      "[2021-06-13 15:02:19 +0000] [4132] [INFO] Booting worker with pid: 4132\n",
      "[\u001b[34m2021-06-13 15:02:30,879\u001b[0m] {\u001b[34mwebserver_command.py:\u001b[0m431} INFO\u001b[0m - Received signal: 2. Closing gunicorn.\u001b[0m\n",
      "[2021-06-13 15:02:30 +0000] [4126] [INFO] Handling signal: int\n",
      "[2021-06-13 15:02:30 +0000] [4132] [INFO] Worker exiting (pid: 4132)\n",
      "[2021-06-13 15:02:30 +0000] [4131] [INFO] Worker exiting (pid: 4131)\n",
      "[2021-06-13 15:02:31 +0000] [4129] [INFO] Worker exiting (pid: 4129)\n",
      "[2021-06-13 15:02:32 +0000] [4130] [INFO] Worker exiting (pid: 4130)\n",
      "[2021-06-13 15:02:32 +0000] [4126] [INFO] Shutting down: Master\n"
     ]
    }
   ],
   "source": [
    "!airflow webserver -p 8081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kCX-riwdpHdL",
    "outputId": "df9b9475-8bdd-483a-fb07-0c43bc2162ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ____________       _____________\n",
      " ____    |__( )_________  __/__  /________      __\n",
      "____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /\n",
      "___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /\n",
      " _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/\n",
      "Starting flask\n",
      " * Serving Flask app \"airflow.utils.serve_logs\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n",
      "[\u001b[34m2021-06-13 14:48:16,879\u001b[0m] {\u001b[34m_internal.py:\u001b[0m122} INFO\u001b[0m -  * Running on \u001b[01mhttp\u001b[22m://\u001b[01m0.0.0.0\u001b[22m:8793/ \u001b[01m(Press CTRL+C to quit)\u001b[22m\u001b[0m\n",
      "[\u001b[34m2021-06-13 14:48:16,887\u001b[0m] {\u001b[34mscheduler_job.py:\u001b[0m1253} INFO\u001b[0m - Starting the scheduler\u001b[0m\n",
      "[\u001b[34m2021-06-13 14:48:16,888\u001b[0m] {\u001b[34mscheduler_job.py:\u001b[0m1258} INFO\u001b[0m - Processing each file at most -1 times\u001b[0m\n",
      "[\u001b[34m2021-06-13 14:48:16,893\u001b[0m] {\u001b[34mdag_processing.py:\u001b[0m254} INFO\u001b[0m - Launched DagFileProcessorManager with pid: 3946\u001b[0m\n",
      "[\u001b[34m2021-06-13 14:48:16,895\u001b[0m] {\u001b[34mscheduler_job.py:\u001b[0m1822} INFO\u001b[0m - Resetting orphaned tasks for active dag runs\u001b[0m\n",
      "[\u001b[34m2021-06-13 14:48:16,901\u001b[0m] {\u001b[34msettings.py:\u001b[0m52} INFO\u001b[0m - Configured default timezone \u001b[01mTimezone('UTC')\u001b[22m\u001b[0m\n",
      "[2021-06-13 14:48:16,918] {dag_processing.py:532} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2 ) when using sqlite. So we set parallelism to 1.\n",
      "[\u001b[34m2021-06-13 14:48:21,691\u001b[0m] {\u001b[34mscheduler_job.py:\u001b[0m748} INFO\u001b[0m - Exiting gracefully upon receiving signal 2\u001b[0m\n",
      "[\u001b[34m2021-06-13 14:48:21,701\u001b[0m] {\u001b[34mscheduler_job.py:\u001b[0m748} INFO\u001b[0m - Exiting gracefully upon receiving signal 2\u001b[0m\n",
      "[\u001b[34m2021-06-13 14:48:22,704\u001b[0m] {\u001b[34mprocess_utils.py:\u001b[0m100} INFO\u001b[0m - Sending Signals.SIGTERM to GPID 3946\u001b[0m\n",
      "[\u001b[34m2021-06-13 14:48:22,836\u001b[0m] {\u001b[34mprocess_utils.py:\u001b[0m66} INFO\u001b[0m - Process \u001b[01mpsutil.Process(pid=3946, status='terminated')\u001b[22m (3946) terminated with exit code 0\u001b[0m\n",
      "[\u001b[34m2021-06-13 14:48:22,838\u001b[0m] {\u001b[34mprocess_utils.py:\u001b[0m100} INFO\u001b[0m - Sending Signals.SIGTERM to GPID 3946\u001b[0m\n",
      "[\u001b[34m2021-06-13 14:48:22,838\u001b[0m] {\u001b[34mscheduler_job.py:\u001b[0m1313} INFO\u001b[0m - Exited execute loop\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!airflow scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LW_616s6Dorm"
   },
   "source": [
    "*Checking the Time According to UTC*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yPJFbMMdKdeG",
    "outputId": "91d4aa3e-d7b2-45f5-d080-15246f54171a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-13 14:48:41.508334+00:00\n"
     ]
    }
   ],
   "source": [
    "from airflow.utils import timezone\n",
    "\n",
    "now = timezone.utcnow()\n",
    "print(now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_Eyanu7x_mr6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DAG.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
